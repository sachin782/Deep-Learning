{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "IxPZQ4N7gEeP",
    "outputId": "535a422c-1efe-47c6-f96c-7b9b3a6ff21e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executed\n"
     ]
    }
   ],
   "source": [
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy\n",
    "print('executed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 710
    },
    "id": "koUityA6nqDg",
    "outputId": "a3f5b65f-b982-4497-cb47-6016cf7d1053"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.    148.     72.    ...  33.6     0.627  50.   ]\n",
      " [  1.     85.     66.    ...  26.6     0.351  31.   ]\n",
      " [  8.    183.     64.    ...  23.3     0.672  32.   ]\n",
      " ...\n",
      " [  5.    121.     72.    ...  26.2     0.245  30.   ]\n",
      " [  1.    126.     60.    ...  30.1     0.349  47.   ]\n",
      " [  1.     93.     70.    ...  30.4     0.315  23.   ]]\n",
      "[1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0.\n",
      " 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0.\n",
      " 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1.\n",
      " 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0.\n",
      " 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1.\n",
      " 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "dataset = loadtxt('pima-indians-diabetes.csv',delimiter = ',')\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cz7mGyHQtIvA"
   },
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "RQdiDndcvTte",
    "outputId": "6430f8fa-ba12-4d90-93a9-7884e385c29d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sachin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Sachin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "accuracy: 75.32%\n",
      "accuracy: 75.32%\n",
      "accuracy: 67.53%\n",
      "accuracy: 83.12%\n",
      "accuracy: 72.73%\n",
      "accuracy: 71.43%\n",
      "accuracy: 74.03%\n",
      "accuracy: 75.32%\n",
      "accuracy: 64.47%\n",
      "accuracy: 72.37%\n",
      "73.16%(+/-4.74%)\n"
     ]
    }
   ],
   "source": [
    "cvscores = []\n",
    "for train,test in kfold.split(X,y):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12,input_dim = 8,activation = 'relu'))\n",
    "    model.add(Dense(8,activation = 'relu'))\n",
    "    model.add(Dense(1,activation = 'sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics = ['accuracy'])\n",
    "    model.fit(X[train],y[train],epochs = 150, batch_size = 10,verbose = 0)\n",
    "    scores = model.evaluate(X[test], y[test], verbose = 0)\n",
    "    print('%s: %.2f%%'%(model.metrics_names[1],scores[1] * 100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print('%.2f%%(+/-%.2f%%)'%(numpy.mean(cvscores),numpy.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size = 0.33, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "id": "aukeQFYkiEsU",
    "outputId": "3ded7cc5-2989-41d2-9361-ba56905e9078"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 344 samples, validate on 170 samples\n",
      "Epoch 1/150\n",
      "344/344 [==============================] - 0s 983us/step - loss: 2.3675 - accuracy: 0.4651 - val_loss: 1.7267 - val_accuracy: 0.5294\n",
      "Epoch 2/150\n",
      "344/344 [==============================] - 0s 171us/step - loss: 1.1797 - accuracy: 0.5610 - val_loss: 0.9784 - val_accuracy: 0.5529\n",
      "Epoch 3/150\n",
      "344/344 [==============================] - 0s 154us/step - loss: 0.8051 - accuracy: 0.5843 - val_loss: 0.8014 - val_accuracy: 0.6882\n",
      "Epoch 4/150\n",
      "344/344 [==============================] - 0s 177us/step - loss: 0.7072 - accuracy: 0.6134 - val_loss: 0.7520 - val_accuracy: 0.6941\n",
      "Epoch 5/150\n",
      "344/344 [==============================] - 0s 159us/step - loss: 0.6866 - accuracy: 0.6279 - val_loss: 0.7337 - val_accuracy: 0.7118\n",
      "Epoch 6/150\n",
      "344/344 [==============================] - 0s 215us/step - loss: 0.6799 - accuracy: 0.6279 - val_loss: 0.7321 - val_accuracy: 0.7000\n",
      "Epoch 7/150\n",
      "344/344 [==============================] - 0s 174us/step - loss: 0.6628 - accuracy: 0.6395 - val_loss: 0.7269 - val_accuracy: 0.7235\n",
      "Epoch 8/150\n",
      "344/344 [==============================] - 0s 162us/step - loss: 0.6476 - accuracy: 0.6686 - val_loss: 0.7327 - val_accuracy: 0.7353\n",
      "Epoch 9/150\n",
      "344/344 [==============================] - 0s 186us/step - loss: 0.6351 - accuracy: 0.6657 - val_loss: 0.7357 - val_accuracy: 0.7059\n",
      "Epoch 10/150\n",
      "344/344 [==============================] - 0s 188us/step - loss: 0.6325 - accuracy: 0.6686 - val_loss: 0.7252 - val_accuracy: 0.7235\n",
      "Epoch 11/150\n",
      "344/344 [==============================] - 0s 226us/step - loss: 0.6364 - accuracy: 0.6744 - val_loss: 0.7329 - val_accuracy: 0.7235\n",
      "Epoch 12/150\n",
      "344/344 [==============================] - 0s 165us/step - loss: 0.6242 - accuracy: 0.6686 - val_loss: 0.7319 - val_accuracy: 0.7235\n",
      "Epoch 13/150\n",
      "344/344 [==============================] - 0s 159us/step - loss: 0.6245 - accuracy: 0.6599 - val_loss: 0.7358 - val_accuracy: 0.7235\n",
      "Epoch 14/150\n",
      "344/344 [==============================] - 0s 194us/step - loss: 0.6213 - accuracy: 0.6628 - val_loss: 0.7362 - val_accuracy: 0.7235\n",
      "Epoch 15/150\n",
      "344/344 [==============================] - 0s 165us/step - loss: 0.6199 - accuracy: 0.6628 - val_loss: 0.7376 - val_accuracy: 0.7176\n",
      "Epoch 16/150\n",
      "344/344 [==============================] - 0s 188us/step - loss: 0.6228 - accuracy: 0.6570 - val_loss: 0.7388 - val_accuracy: 0.7235\n",
      "Epoch 17/150\n",
      "344/344 [==============================] - 0s 157us/step - loss: 0.6216 - accuracy: 0.6570 - val_loss: 0.7362 - val_accuracy: 0.7118\n",
      "Epoch 18/150\n",
      "344/344 [==============================] - 0s 249us/step - loss: 0.6132 - accuracy: 0.6715 - val_loss: 0.7343 - val_accuracy: 0.7294\n",
      "Epoch 19/150\n",
      "344/344 [==============================] - 0s 180us/step - loss: 0.6091 - accuracy: 0.6599 - val_loss: 0.7262 - val_accuracy: 0.7176\n",
      "Epoch 20/150\n",
      "344/344 [==============================] - 0s 162us/step - loss: 0.6114 - accuracy: 0.6744 - val_loss: 0.7397 - val_accuracy: 0.7176\n",
      "Epoch 21/150\n",
      "344/344 [==============================] - 0s 177us/step - loss: 0.6089 - accuracy: 0.6686 - val_loss: 0.7349 - val_accuracy: 0.7059\n",
      "Epoch 22/150\n",
      "344/344 [==============================] - 0s 159us/step - loss: 0.6067 - accuracy: 0.6715 - val_loss: 0.7234 - val_accuracy: 0.7059\n",
      "Epoch 23/150\n",
      "344/344 [==============================] - 0s 177us/step - loss: 0.6099 - accuracy: 0.6657 - val_loss: 0.7269 - val_accuracy: 0.7118\n",
      "Epoch 24/150\n",
      "344/344 [==============================] - 0s 159us/step - loss: 0.6062 - accuracy: 0.6715 - val_loss: 0.7247 - val_accuracy: 0.7118\n",
      "Epoch 25/150\n",
      "344/344 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.70 - 0s 162us/step - loss: 0.6050 - accuracy: 0.6744 - val_loss: 0.7278 - val_accuracy: 0.7059\n",
      "Epoch 26/150\n",
      "344/344 [==============================] - 0s 188us/step - loss: 0.6060 - accuracy: 0.6715 - val_loss: 0.7266 - val_accuracy: 0.7176\n",
      "Epoch 27/150\n",
      "344/344 [==============================] - 0s 220us/step - loss: 0.6028 - accuracy: 0.6773 - val_loss: 0.7211 - val_accuracy: 0.7059\n",
      "Epoch 28/150\n",
      "344/344 [==============================] - 0s 180us/step - loss: 0.5983 - accuracy: 0.6773 - val_loss: 0.7219 - val_accuracy: 0.7059\n",
      "Epoch 29/150\n",
      "344/344 [==============================] - 0s 188us/step - loss: 0.6059 - accuracy: 0.6686 - val_loss: 0.7281 - val_accuracy: 0.7118\n",
      "Epoch 30/150\n",
      "344/344 [==============================] - 0s 162us/step - loss: 0.5990 - accuracy: 0.6802 - val_loss: 0.7208 - val_accuracy: 0.7118\n",
      "Epoch 31/150\n",
      "344/344 [==============================] - 0s 232us/step - loss: 0.6002 - accuracy: 0.6773 - val_loss: 0.7223 - val_accuracy: 0.7176\n",
      "Epoch 32/150\n",
      "344/344 [==============================] - 0s 191us/step - loss: 0.5956 - accuracy: 0.6686 - val_loss: 0.7246 - val_accuracy: 0.7176\n",
      "Epoch 33/150\n",
      "344/344 [==============================] - 0s 186us/step - loss: 0.5960 - accuracy: 0.6715 - val_loss: 0.7161 - val_accuracy: 0.7176\n",
      "Epoch 34/150\n",
      "344/344 [==============================] - 0s 180us/step - loss: 0.5930 - accuracy: 0.6744 - val_loss: 0.7172 - val_accuracy: 0.7118\n",
      "Epoch 35/150\n",
      "344/344 [==============================] - 0s 183us/step - loss: 0.5948 - accuracy: 0.6773 - val_loss: 0.7230 - val_accuracy: 0.7118\n",
      "Epoch 36/150\n",
      "344/344 [==============================] - 0s 238us/step - loss: 0.5921 - accuracy: 0.6802 - val_loss: 0.7142 - val_accuracy: 0.7176\n",
      "Epoch 37/150\n",
      "344/344 [==============================] - 0s 191us/step - loss: 0.5934 - accuracy: 0.6744 - val_loss: 0.7155 - val_accuracy: 0.7118\n",
      "Epoch 38/150\n",
      "344/344 [==============================] - 0s 183us/step - loss: 0.5938 - accuracy: 0.6715 - val_loss: 0.7129 - val_accuracy: 0.7176\n",
      "Epoch 39/150\n",
      "344/344 [==============================] - 0s 168us/step - loss: 0.5933 - accuracy: 0.6773 - val_loss: 0.7160 - val_accuracy: 0.7176\n",
      "Epoch 40/150\n",
      "344/344 [==============================] - 0s 267us/step - loss: 0.5896 - accuracy: 0.6831 - val_loss: 0.7173 - val_accuracy: 0.7118\n",
      "Epoch 41/150\n",
      "344/344 [==============================] - 0s 232us/step - loss: 0.5945 - accuracy: 0.6890 - val_loss: 0.7170 - val_accuracy: 0.7118\n",
      "Epoch 42/150\n",
      "344/344 [==============================] - 0s 261us/step - loss: 0.5939 - accuracy: 0.6773 - val_loss: 0.7151 - val_accuracy: 0.7235\n",
      "Epoch 43/150\n",
      "344/344 [==============================] - 0s 220us/step - loss: 0.5902 - accuracy: 0.6744 - val_loss: 0.7174 - val_accuracy: 0.7118\n",
      "Epoch 44/150\n",
      "344/344 [==============================] - 0s 264us/step - loss: 0.5914 - accuracy: 0.6773 - val_loss: 0.7090 - val_accuracy: 0.7118\n",
      "Epoch 45/150\n",
      "344/344 [==============================] - 0s 273us/step - loss: 0.5908 - accuracy: 0.6802 - val_loss: 0.7142 - val_accuracy: 0.7059\n",
      "Epoch 46/150\n",
      "344/344 [==============================] - 0s 264us/step - loss: 0.5866 - accuracy: 0.6802 - val_loss: 0.7101 - val_accuracy: 0.7176\n",
      "Epoch 47/150\n",
      "344/344 [==============================] - 0s 281us/step - loss: 0.5876 - accuracy: 0.6715 - val_loss: 0.7314 - val_accuracy: 0.7118\n",
      "Epoch 48/150\n",
      "344/344 [==============================] - 0s 281us/step - loss: 0.5951 - accuracy: 0.6744 - val_loss: 0.7100 - val_accuracy: 0.7176\n",
      "Epoch 49/150\n",
      "344/344 [==============================] - 0s 258us/step - loss: 0.5913 - accuracy: 0.6802 - val_loss: 0.7172 - val_accuracy: 0.7176\n",
      "Epoch 50/150\n",
      "344/344 [==============================] - 0s 258us/step - loss: 0.5903 - accuracy: 0.6773 - val_loss: 0.7226 - val_accuracy: 0.7118\n",
      "Epoch 51/150\n",
      "344/344 [==============================] - 0s 258us/step - loss: 0.5872 - accuracy: 0.6831 - val_loss: 0.7084 - val_accuracy: 0.7176\n",
      "Epoch 52/150\n",
      "344/344 [==============================] - 0s 275us/step - loss: 0.5862 - accuracy: 0.6744 - val_loss: 0.7178 - val_accuracy: 0.7059\n",
      "Epoch 53/150\n",
      "344/344 [==============================] - 0s 226us/step - loss: 0.5862 - accuracy: 0.6802 - val_loss: 0.7086 - val_accuracy: 0.7235\n",
      "Epoch 54/150\n",
      "344/344 [==============================] - 0s 251us/step - loss: 0.5845 - accuracy: 0.6831 - val_loss: 0.7298 - val_accuracy: 0.7118\n",
      "Epoch 55/150\n",
      "344/344 [==============================] - 0s 191us/step - loss: 0.5825 - accuracy: 0.6802 - val_loss: 0.7059 - val_accuracy: 0.7176\n",
      "Epoch 56/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344/344 [==============================] - 0s 183us/step - loss: 0.5852 - accuracy: 0.6744 - val_loss: 0.7195 - val_accuracy: 0.7059\n",
      "Epoch 57/150\n",
      "344/344 [==============================] - 0s 255us/step - loss: 0.5823 - accuracy: 0.6831 - val_loss: 0.7014 - val_accuracy: 0.7235\n",
      "Epoch 58/150\n",
      "344/344 [==============================] - 0s 284us/step - loss: 0.5797 - accuracy: 0.6860 - val_loss: 0.7175 - val_accuracy: 0.7059\n",
      "Epoch 59/150\n",
      "344/344 [==============================] - 0s 296us/step - loss: 0.5808 - accuracy: 0.6890 - val_loss: 0.7154 - val_accuracy: 0.7176\n",
      "Epoch 60/150\n",
      "344/344 [==============================] - 0s 293us/step - loss: 0.5811 - accuracy: 0.6744 - val_loss: 0.7200 - val_accuracy: 0.7118\n",
      "Epoch 61/150\n",
      "344/344 [==============================] - 0s 220us/step - loss: 0.5828 - accuracy: 0.6831 - val_loss: 0.7225 - val_accuracy: 0.7059\n",
      "Epoch 62/150\n",
      "344/344 [==============================] - 0s 255us/step - loss: 0.5808 - accuracy: 0.6773 - val_loss: 0.7125 - val_accuracy: 0.7176\n",
      "Epoch 63/150\n",
      "344/344 [==============================] - 0s 191us/step - loss: 0.5817 - accuracy: 0.6773 - val_loss: 0.7264 - val_accuracy: 0.7059\n",
      "Epoch 64/150\n",
      "344/344 [==============================] - 0s 238us/step - loss: 0.5853 - accuracy: 0.6831 - val_loss: 0.7122 - val_accuracy: 0.7118\n",
      "Epoch 65/150\n",
      "344/344 [==============================] - 0s 186us/step - loss: 0.5785 - accuracy: 0.6802 - val_loss: 0.7054 - val_accuracy: 0.7176\n",
      "Epoch 66/150\n",
      "344/344 [==============================] - 0s 235us/step - loss: 0.5811 - accuracy: 0.6831 - val_loss: 0.7228 - val_accuracy: 0.7118\n",
      "Epoch 67/150\n",
      "344/344 [==============================] - 0s 183us/step - loss: 0.5785 - accuracy: 0.6744 - val_loss: 0.7188 - val_accuracy: 0.7118\n",
      "Epoch 68/150\n",
      "344/344 [==============================] - 0s 232us/step - loss: 0.5772 - accuracy: 0.6919 - val_loss: 0.7115 - val_accuracy: 0.7176\n",
      "Epoch 69/150\n",
      "344/344 [==============================] - 0s 183us/step - loss: 0.5796 - accuracy: 0.6715 - val_loss: 0.7225 - val_accuracy: 0.7118\n",
      "Epoch 70/150\n",
      "344/344 [==============================] - 0s 177us/step - loss: 0.5828 - accuracy: 0.6715 - val_loss: 0.7190 - val_accuracy: 0.7118\n",
      "Epoch 71/150\n",
      "344/344 [==============================] - 0s 180us/step - loss: 0.5812 - accuracy: 0.6860 - val_loss: 0.7161 - val_accuracy: 0.7118\n",
      "Epoch 72/150\n",
      "344/344 [==============================] - 0s 206us/step - loss: 0.5754 - accuracy: 0.6831 - val_loss: 0.7135 - val_accuracy: 0.7176\n",
      "Epoch 73/150\n",
      "344/344 [==============================] - 0s 310us/step - loss: 0.5758 - accuracy: 0.6831 - val_loss: 0.7295 - val_accuracy: 0.7059\n",
      "Epoch 74/150\n",
      "344/344 [==============================] - 0s 412us/step - loss: 0.5767 - accuracy: 0.6802 - val_loss: 0.7126 - val_accuracy: 0.7176\n",
      "Epoch 75/150\n",
      "344/344 [==============================] - 0s 302us/step - loss: 0.5798 - accuracy: 0.6744 - val_loss: 0.7217 - val_accuracy: 0.7176\n",
      "Epoch 76/150\n",
      "344/344 [==============================] - 0s 290us/step - loss: 0.5795 - accuracy: 0.6802 - val_loss: 0.7178 - val_accuracy: 0.7176\n",
      "Epoch 77/150\n",
      "344/344 [==============================] - 0s 287us/step - loss: 0.5779 - accuracy: 0.6802 - val_loss: 0.7274 - val_accuracy: 0.7294\n",
      "Epoch 78/150\n",
      "344/344 [==============================] - 0s 316us/step - loss: 0.5783 - accuracy: 0.6773 - val_loss: 0.7328 - val_accuracy: 0.7118\n",
      "Epoch 79/150\n",
      "344/344 [==============================] - 0s 258us/step - loss: 0.5776 - accuracy: 0.6860 - val_loss: 0.7295 - val_accuracy: 0.7118\n",
      "Epoch 80/150\n",
      "344/344 [==============================] - 0s 299us/step - loss: 0.5716 - accuracy: 0.6831 - val_loss: 0.7275 - val_accuracy: 0.7176\n",
      "Epoch 81/150\n",
      "344/344 [==============================] - 0s 304us/step - loss: 0.5727 - accuracy: 0.6773 - val_loss: 0.7196 - val_accuracy: 0.7235\n",
      "Epoch 82/150\n",
      "344/344 [==============================] - 0s 307us/step - loss: 0.5758 - accuracy: 0.6802 - val_loss: 0.7263 - val_accuracy: 0.7235\n",
      "Epoch 83/150\n",
      "344/344 [==============================] - 0s 304us/step - loss: 0.5793 - accuracy: 0.6860 - val_loss: 0.7481 - val_accuracy: 0.7235\n",
      "Epoch 84/150\n",
      "344/344 [==============================] - 0s 328us/step - loss: 0.5761 - accuracy: 0.6744 - val_loss: 0.7131 - val_accuracy: 0.7235\n",
      "Epoch 85/150\n",
      "344/344 [==============================] - 0s 310us/step - loss: 0.5754 - accuracy: 0.6802 - val_loss: 0.7174 - val_accuracy: 0.7353\n",
      "Epoch 86/150\n",
      "344/344 [==============================] - 0s 310us/step - loss: 0.5720 - accuracy: 0.6860 - val_loss: 0.7352 - val_accuracy: 0.7294\n",
      "Epoch 87/150\n",
      "344/344 [==============================] - 0s 293us/step - loss: 0.5733 - accuracy: 0.6802 - val_loss: 0.7154 - val_accuracy: 0.7353\n",
      "Epoch 88/150\n",
      "344/344 [==============================] - 0s 241us/step - loss: 0.5807 - accuracy: 0.6773 - val_loss: 0.7205 - val_accuracy: 0.7353\n",
      "Epoch 89/150\n",
      "344/344 [==============================] - 0s 316us/step - loss: 0.5764 - accuracy: 0.6773 - val_loss: 0.7251 - val_accuracy: 0.7235\n",
      "Epoch 90/150\n",
      "344/344 [==============================] - 0s 304us/step - loss: 0.5755 - accuracy: 0.6773 - val_loss: 0.7417 - val_accuracy: 0.7176\n",
      "Epoch 91/150\n",
      "344/344 [==============================] - 0s 299us/step - loss: 0.5739 - accuracy: 0.6919 - val_loss: 0.7398 - val_accuracy: 0.7294\n",
      "Epoch 92/150\n",
      "344/344 [==============================] - 0s 293us/step - loss: 0.5694 - accuracy: 0.6860 - val_loss: 0.7160 - val_accuracy: 0.7353\n",
      "Epoch 93/150\n",
      "344/344 [==============================] - 0s 278us/step - loss: 0.5746 - accuracy: 0.6919 - val_loss: 0.7399 - val_accuracy: 0.7353\n",
      "Epoch 94/150\n",
      "344/344 [==============================] - 0s 302us/step - loss: 0.5863 - accuracy: 0.6744 - val_loss: 0.7233 - val_accuracy: 0.7353\n",
      "Epoch 95/150\n",
      "344/344 [==============================] - 0s 316us/step - loss: 0.5662 - accuracy: 0.6919 - val_loss: 0.7345 - val_accuracy: 0.7294\n",
      "Epoch 96/150\n",
      "344/344 [==============================] - 0s 293us/step - loss: 0.5693 - accuracy: 0.6890 - val_loss: 0.7393 - val_accuracy: 0.7294\n",
      "Epoch 97/150\n",
      "344/344 [==============================] - 0s 336us/step - loss: 0.5698 - accuracy: 0.6860 - val_loss: 0.7274 - val_accuracy: 0.7412\n",
      "Epoch 98/150\n",
      "344/344 [==============================] - 0s 302us/step - loss: 0.5661 - accuracy: 0.6860 - val_loss: 0.7351 - val_accuracy: 0.7294\n",
      "Epoch 99/150\n",
      "344/344 [==============================] - 0s 264us/step - loss: 0.5697 - accuracy: 0.6831 - val_loss: 0.7363 - val_accuracy: 0.7294\n",
      "Epoch 100/150\n",
      "344/344 [==============================] - 0s 302us/step - loss: 0.5746 - accuracy: 0.6744 - val_loss: 0.7527 - val_accuracy: 0.7353\n",
      "Epoch 101/150\n",
      "344/344 [==============================] - 0s 304us/step - loss: 0.5683 - accuracy: 0.6860 - val_loss: 0.7529 - val_accuracy: 0.7118\n",
      "Epoch 102/150\n",
      "344/344 [==============================] - 0s 296us/step - loss: 0.5685 - accuracy: 0.6919 - val_loss: 0.7420 - val_accuracy: 0.7294\n",
      "Epoch 103/150\n",
      "344/344 [==============================] - 0s 313us/step - loss: 0.5618 - accuracy: 0.6919 - val_loss: 0.7375 - val_accuracy: 0.7235\n",
      "Epoch 104/150\n",
      "344/344 [==============================] - 0s 302us/step - loss: 0.5619 - accuracy: 0.6890 - val_loss: 0.7403 - val_accuracy: 0.7176\n",
      "Epoch 105/150\n",
      "344/344 [==============================] - 0s 305us/step - loss: 0.5634 - accuracy: 0.6977 - val_loss: 0.7541 - val_accuracy: 0.7294\n",
      "Epoch 106/150\n",
      "344/344 [==============================] - 0s 313us/step - loss: 0.5641 - accuracy: 0.6977 - val_loss: 0.7524 - val_accuracy: 0.7176\n",
      "Epoch 107/150\n",
      "344/344 [==============================] - 0s 284us/step - loss: 0.5713 - accuracy: 0.6860 - val_loss: 0.7853 - val_accuracy: 0.7059\n",
      "Epoch 108/150\n",
      "344/344 [==============================] - 0s 307us/step - loss: 0.5655 - accuracy: 0.7006 - val_loss: 0.7487 - val_accuracy: 0.7235\n",
      "Epoch 109/150\n",
      "344/344 [==============================] - 0s 275us/step - loss: 0.5624 - accuracy: 0.6890 - val_loss: 0.7658 - val_accuracy: 0.7176\n",
      "Epoch 110/150\n",
      "344/344 [==============================] - 0s 296us/step - loss: 0.5573 - accuracy: 0.6890 - val_loss: 0.7606 - val_accuracy: 0.7294\n",
      "Epoch 111/150\n",
      "344/344 [==============================] - 0s 281us/step - loss: 0.5680 - accuracy: 0.6860 - val_loss: 0.7667 - val_accuracy: 0.7235\n",
      "Epoch 112/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344/344 [==============================] - 0s 299us/step - loss: 0.5624 - accuracy: 0.6890 - val_loss: 0.7706 - val_accuracy: 0.7294\n",
      "Epoch 113/150\n",
      "344/344 [==============================] - 0s 246us/step - loss: 0.5579 - accuracy: 0.6977 - val_loss: 0.7672 - val_accuracy: 0.7294\n",
      "Epoch 114/150\n",
      "344/344 [==============================] - 0s 302us/step - loss: 0.5649 - accuracy: 0.6890 - val_loss: 0.7817 - val_accuracy: 0.7294\n",
      "Epoch 115/150\n",
      "344/344 [==============================] - 0s 316us/step - loss: 0.5558 - accuracy: 0.7006 - val_loss: 0.7822 - val_accuracy: 0.7176\n",
      "Epoch 116/150\n",
      "344/344 [==============================] - 0s 339us/step - loss: 0.5640 - accuracy: 0.6948 - val_loss: 0.7806 - val_accuracy: 0.7176\n",
      "Epoch 117/150\n",
      "344/344 [==============================] - 0s 351us/step - loss: 0.5640 - accuracy: 0.6948 - val_loss: 0.7809 - val_accuracy: 0.7235\n",
      "Epoch 118/150\n",
      "344/344 [==============================] - 0s 319us/step - loss: 0.5553 - accuracy: 0.7006 - val_loss: 0.7763 - val_accuracy: 0.7294\n",
      "Epoch 119/150\n",
      "344/344 [==============================] - 0s 362us/step - loss: 0.5608 - accuracy: 0.7064 - val_loss: 0.7818 - val_accuracy: 0.7235\n",
      "Epoch 120/150\n",
      "344/344 [==============================] - 0s 333us/step - loss: 0.5581 - accuracy: 0.6948 - val_loss: 0.7889 - val_accuracy: 0.7235\n",
      "Epoch 121/150\n",
      "344/344 [==============================] - 0s 307us/step - loss: 0.5647 - accuracy: 0.6831 - val_loss: 0.7763 - val_accuracy: 0.7353\n",
      "Epoch 122/150\n",
      "344/344 [==============================] - 0s 345us/step - loss: 0.5559 - accuracy: 0.7064 - val_loss: 0.7976 - val_accuracy: 0.7235\n",
      "Epoch 123/150\n",
      "344/344 [==============================] - 0s 307us/step - loss: 0.5574 - accuracy: 0.6977 - val_loss: 0.8059 - val_accuracy: 0.7235\n",
      "Epoch 124/150\n",
      "344/344 [==============================] - 0s 342us/step - loss: 0.5529 - accuracy: 0.6977 - val_loss: 0.8227 - val_accuracy: 0.7176\n",
      "Epoch 125/150\n",
      "344/344 [==============================] - 0s 360us/step - loss: 0.5559 - accuracy: 0.7006 - val_loss: 0.7845 - val_accuracy: 0.7235\n",
      "Epoch 126/150\n",
      "344/344 [==============================] - 0s 313us/step - loss: 0.5562 - accuracy: 0.7064 - val_loss: 0.7907 - val_accuracy: 0.7294\n",
      "Epoch 127/150\n",
      "344/344 [==============================] - 0s 287us/step - loss: 0.5511 - accuracy: 0.7035 - val_loss: 0.8073 - val_accuracy: 0.7235\n",
      "Epoch 128/150\n",
      "344/344 [==============================] - 0s 296us/step - loss: 0.5514 - accuracy: 0.6919 - val_loss: 0.8039 - val_accuracy: 0.7353\n",
      "Epoch 129/150\n",
      "344/344 [==============================] - 0s 290us/step - loss: 0.5525 - accuracy: 0.6977 - val_loss: 0.8001 - val_accuracy: 0.7353\n",
      "Epoch 130/150\n",
      "344/344 [==============================] - 0s 273us/step - loss: 0.5543 - accuracy: 0.6860 - val_loss: 0.7903 - val_accuracy: 0.7235\n",
      "Epoch 131/150\n",
      "344/344 [==============================] - 0s 287us/step - loss: 0.5586 - accuracy: 0.7035 - val_loss: 0.8153 - val_accuracy: 0.7235\n",
      "Epoch 132/150\n",
      "344/344 [==============================] - 0s 275us/step - loss: 0.5484 - accuracy: 0.7035 - val_loss: 0.8424 - val_accuracy: 0.7235\n",
      "Epoch 133/150\n",
      "344/344 [==============================] - 0s 304us/step - loss: 0.5678 - accuracy: 0.7006 - val_loss: 0.8262 - val_accuracy: 0.7353\n",
      "Epoch 134/150\n",
      "344/344 [==============================] - 0s 290us/step - loss: 0.5637 - accuracy: 0.7006 - val_loss: 0.8078 - val_accuracy: 0.7294\n",
      "Epoch 135/150\n",
      "344/344 [==============================] - 0s 302us/step - loss: 0.5492 - accuracy: 0.7064 - val_loss: 0.8460 - val_accuracy: 0.7176\n",
      "Epoch 136/150\n",
      "344/344 [==============================] - 0s 293us/step - loss: 0.5490 - accuracy: 0.7122 - val_loss: 0.8123 - val_accuracy: 0.7235\n",
      "Epoch 137/150\n",
      "344/344 [==============================] - 0s 299us/step - loss: 0.5534 - accuracy: 0.7064 - val_loss: 0.8321 - val_accuracy: 0.7235\n",
      "Epoch 138/150\n",
      "344/344 [==============================] - 0s 313us/step - loss: 0.5510 - accuracy: 0.6977 - val_loss: 0.8249 - val_accuracy: 0.7294\n",
      "Epoch 139/150\n",
      "344/344 [==============================] - 0s 246us/step - loss: 0.5509 - accuracy: 0.7035 - val_loss: 0.8431 - val_accuracy: 0.7294\n",
      "Epoch 140/150\n",
      "344/344 [==============================] - 0s 293us/step - loss: 0.5490 - accuracy: 0.7064 - val_loss: 0.8380 - val_accuracy: 0.7235\n",
      "Epoch 141/150\n",
      "344/344 [==============================] - 0s 287us/step - loss: 0.5491 - accuracy: 0.7093 - val_loss: 0.8312 - val_accuracy: 0.7294\n",
      "Epoch 142/150\n",
      "344/344 [==============================] - 0s 304us/step - loss: 0.5529 - accuracy: 0.7006 - val_loss: 0.8320 - val_accuracy: 0.7235\n",
      "Epoch 143/150\n",
      "344/344 [==============================] - 0s 293us/step - loss: 0.5519 - accuracy: 0.7035 - val_loss: 0.8595 - val_accuracy: 0.7176\n",
      "Epoch 144/150\n",
      "344/344 [==============================] - 0s 302us/step - loss: 0.5497 - accuracy: 0.6977 - val_loss: 0.8397 - val_accuracy: 0.7294\n",
      "Epoch 145/150\n",
      "344/344 [==============================] - 0s 310us/step - loss: 0.5503 - accuracy: 0.7093 - val_loss: 0.8289 - val_accuracy: 0.7235\n",
      "Epoch 146/150\n",
      "344/344 [==============================] - 0s 296us/step - loss: 0.5462 - accuracy: 0.7064 - val_loss: 0.8541 - val_accuracy: 0.7235\n",
      "Epoch 147/150\n",
      "344/344 [==============================] - 0s 307us/step - loss: 0.5421 - accuracy: 0.7122 - val_loss: 0.8529 - val_accuracy: 0.7235\n",
      "Epoch 148/150\n",
      "344/344 [==============================] - 0s 244us/step - loss: 0.5465 - accuracy: 0.7064 - val_loss: 0.8707 - val_accuracy: 0.7294\n",
      "Epoch 149/150\n",
      "344/344 [==============================] - 0s 241us/step - loss: 0.5446 - accuracy: 0.7064 - val_loss: 0.8646 - val_accuracy: 0.7235\n",
      "Epoch 150/150\n",
      "344/344 [==============================] - 0s 203us/step - loss: 0.5436 - accuracy: 0.7035 - val_loss: 0.8688 - val_accuracy: 0.7235\n",
      "254/254 [==============================] - 0s 43us/step\n",
      "Accuracy: 66.54\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12,input_dim = 8,activation = 'relu'))\n",
    "model.add(Dense(8,activation = 'relu'))\n",
    "model.add(Dense(1,activation = 'sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics = ['accuracy'])\n",
    "model.fit(X_train,y_train,validation_split = 0.33,epochs = 150,batch_size = 10)\n",
    "_,accuracy = model.evaluate(X_test,y_test)\n",
    "print('Accuracy: %.2f'%(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "sdrZ8IOCobaB",
    "outputId": "0f28b787-8300-409b-8cd1-ecccbe4a9f46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00888924]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "Xp = array([[1.0, 85.0, 66.0, 29.0, 0.0, 26.6, 0.351, 31.0]])\n",
    "yp = model.predict(Xp)\n",
    "print(yp)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
